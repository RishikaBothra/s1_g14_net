\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\title{CSE400: Fundamentals of Probability in Computing \\ Lecture 8: Gaussian, Uniform, Exponential, and Gamma Random Variables}
\author{Lecture Scribe}
\date{January 29, 2026}

\begin{document}

\maketitle

\section{Continuous Random Variables: Moments}

\subsection{Central Moments}
The $n$-th order central moment is defined as the expectation of the random variable centered around its mean.
\begin{equation}
    [cite_start]E[(x - \mu_X)^n] = \int_{-\infty}^{\infty} (x - \mu_X)^n f_X(x) dx \quad \text{[cite: 20, 51]}
\end{equation}

\textbf{Specific Moments:}
\begin{itemize}
    [cite_start]\item \textbf{Zeroth Moment ($n=0$):} $E[(x - \mu_X)^0] = E[1] = 1$[cite: 55].
    [cite_start]\item \textbf{First Moment ($n=1$):} $E[(x - \mu_X)] = E[x] - \mu_x = 0$[cite: 56, 58].
    \item \textbf{Second Moment ($n=2$):} Defined as the Variance ($\sigma_X^2$).
    \begin{equation}
        [cite_start]\sigma_X^2 = E[(x - \mu_X)^2] = E[x^2] - \mu_X^2 \quad \text{[cite: 59, 61]}
    \end{equation}
\end{itemize}

\subsection{Higher Order Moments (Shape)}
\begin{itemize}
    \item \textbf{Skewness ($n=3$):} A measure of the symmetry of the Probability Density Function (PDF).
    \begin{equation}
        [cite_start]C_s = \frac{E[(X - \mu_X)^3]}{\sigma_X^3} \quad \text{[cite: 81]}
    \end{equation}
    \begin{itemize}
        [cite_start]\item If $C_s$ is positive (+), the PDF is Right Skewed[cite: 82].
        [cite_start]\item If $C_s$ is negative (-), the PDF is Left Skewed[cite: 83].
    \end{itemize}
    
    \item \textbf{Kurtosis ($n=4$):} A measure of the "tailedness" or peak of the distribution.
    \begin{equation}
        [cite_start]C_k = \frac{E[(X - \mu_X)^4]}{\sigma^4} \quad \text{[cite: 86, 341]}
    \end{equation}
    [cite_start]A large value indicates the random variable will have a large peak near the mean[cite: 88, 89].
\end{itemize}

\section{Theorems on Expectation}

\subsection{Linearity of Expectation}
For any constants $a$ and $b$:
\begin{equation}
    [cite_start]E[ax + b] = a E[x] + b \quad \text{[cite: 94]}
\end{equation}

\subsection{Sum of Functions}
For a sum of several functions $g(x) = g_1(x) + g_2(x) + \dots + g_N(x)$:
\begin{equation}
    [cite_start]E\left[\sum_{k=1}^{N} g_k(x)\right] = \sum_{k=1}^{N} E[g_k(x)] \quad \text{[cite: 97]}
\end{equation}

\section{Gaussian Random Variable}

\subsection{Definition}
A Gaussian random variable $X \sim \mathcal{N}(m, \sigma^2)$ is defined by the PDF:
\begin{equation}
    [cite_start]f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - m)^2}{2\sigma^2}\right) \quad \text{[cite: 119]}
\end{equation}
Where $m$ is the mean and $\sigma^2$ is the variance. [cite_start]If $m=0$ and $\sigma^2=1$, it is a Standard Normal Distribution[cite: 121].

\subsection{Standard Functions}
Several standard integrals are used to evaluate Gaussian probabilities:
\begin{itemize}
    \item \textbf{Error Function:} 
    [cite_start]$erf(x) = \frac{2}{\sqrt{\pi}}\int_{0}^{x} \exp(-t^2) dt$[cite: 166].
    \item \textbf{Complementary Error Function:} 
    [cite_start]$erfc(x) = 1 - erf(x) = \frac{2}{\sqrt{\pi}}\int_{x}^{\infty} \exp(-t^2) dt$[cite: 167].
    \item \textbf{$\Phi$-function (CDF of Standard Normal):} Represents the area under the left tail.
    \begin{equation}
        [cite_start]\Phi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} \exp\left(-\frac{t^2}{2}\right) dt \quad \text{[cite: 175, 250]}
    \end{equation}
    \item \textbf{Q-function (Gaussian Tail Function):} Represents the area under the right tail.
    \begin{equation}
        [cite_start]Q(x) = \frac{1}{\sqrt{2\pi}} \int_{x}^{\infty} \exp\left(-\frac{t^2}{2}\right) dt \quad \text{[cite: 176, 254]}
    \end{equation}
\end{itemize}

\subsection{Key Identities and Relations}
\begin{itemize}
    [cite_start]\item $Q(x) = 1 - \Phi(x)$[cite: 214].
    [cite_start]\item $\Phi(-x) = 1 - \Phi(x)$[cite: 305].
    \item The CDF of a general Gaussian variable can be expressed as:
    \begin{equation}
        [cite_start]F_X(x) = \Phi\left(\frac{x - m}{\sigma}\right) = 1 - Q\left(\frac{x - m}{\sigma}\right) \quad \text{[cite: 211, 214]}
    \end{equation}
    \item Tail probabilities for a general Gaussian variable:
    \begin{equation}
        [cite_start]Pr(X > x) = Q\left(\frac{x - m}{\sigma}\right) \quad \text{[cite: 205]}
    \end{equation}
\end{itemize}

\section{Worked Example 1: Gaussian Probabilities}
\textbf{Problem:}
A random variable $X$ has a PDF given by:
$$ f_X(x) = \frac{1}{\sqrt{8\pi}} \exp\left(-\frac{(x+3)^2}{8}\right) $$
[cite_start]Find probabilities in terms of Q-functions[cite: 262].

\textbf{Solution:}
\begin{enumerate}
    \item \textbf{Identify parameters:}
    Comparing to the standard form $\frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{(x-m)^2}{2\sigma^2})$, we get:
    [cite_start]$$ m = -3, \quad \sigma^2 = 4 \Rightarrow \sigma = 2 \quad \text{[cite: 287]} $$

    \item \textbf{Calculate $Pr(X \le 0)$:}
    [cite_start]$$ Pr(X \le 0) = F_X(0) = \Phi\left(\frac{0 - (-3)}{2}\right) = \Phi(1.5) = 1 - Q(1.5) \quad \text{[cite: 297]} $$

    \item \textbf{Calculate $Pr(X > 4)$:}
    [cite_start]$$ Pr(X > 4) = Q\left(\frac{4 - (-3)}{2}\right) = Q(3.5) \quad \text{[cite: 298]} $$

    \item \textbf{Calculate $Pr(|X+3| < 2)$:}
    $$ |X+3| [cite_start]< 2 \Rightarrow -2 < X+3 < 2 \Rightarrow -5 < X < -1 \quad \text{[cite: 303]} $$
    $$ Pr(-5 < X < -1) = F_X(-1) - F_X(-5) = \Phi(1) - \Phi(-1) $$
    Using $\Phi(-1) = 1 - \Phi(1)$:
    [cite_start]$$ = \Phi(1) - (1 - \Phi(1)) = 2\Phi(1) - 1 = 1 - 2Q(1) \quad \text{[cite: 307]} $$

    \item \textbf{Calculate $Pr(|X-2| > 1)$:}
    $$ |X-2| > [cite_start]1 \Rightarrow X < 1 \text{ or } X > 3 \quad \text{[cite: 308]} $$
    $$ Pr = F_X(1) + Pr(X > 3) $$
    $$ F_X(1) = \Phi\left(\frac{1 - (-3)}{2}\right) = \Phi(2) = 1 - Q(2) $$
    $$ Pr(X > 3) = Q\left(\frac{3 - (-3)}{2}\right) = Q(3) $$
    [cite_start]$$ \text{Total} = 1 - Q(2) + Q(3) \quad \text{[cite: 309]} $$
\end{enumerate}

\section{Worked Example 2: CDF Analysis}
\textbf{Problem:}
The CDF of a random variable is given by:
$$ F_X(x) = \begin{cases} 0, & x < 0 \\ x^2, & 0 \le x \le 1 \\ 1, & x > 1 \end{cases} $$
[cite_start]Find the Mean, Variance, Skewness, and Kurtosis[cite: 322, 323].

\textbf{Solution:}
\begin{enumerate}
    \item \textbf{Find PDF:}
    [cite_start]$$ f_X(x) = \frac{d}{dx}F_X(x) = 2x, \quad 0 \le x \le 1 \quad \text{[cite: 333]} $$
    
    \item \textbf{Calculate Mean ($\mu_X$):}
    [cite_start]$$ \mu_X = \int_{0}^{1} x(2x) dx = \int_{0}^{1} 2x^2 dx = \left[\frac{2x^3}{3}\right]_0^1 = \frac{2}{3} \quad \text{[cite: 333]} $$
    
    \item \textbf{Calculate Variance ($\sigma_X^2$):}
    $$ \sigma_X^2 = \int_{0}^{1} x^2(2x) dx - \mu_X^2 = \int_{0}^{1} 2x^3 dx - \left(\frac{2}{3}\right)^2 $$
    [cite_start]$$ = \left[\frac{2x^4}{4}\right]_0^1 - \frac{4}{9} = \frac{1}{2} - \frac{4}{9} = \frac{9-8}{18} = \frac{1}{18} \quad \text{[cite: 340]} $$
    
    \item \textbf{Skewness and Kurtosis:}
    [cite_start]$$ C_s = \frac{E[(X-\mu)^3]}{\sigma^3} = -\frac{2\sqrt{2}}{5} \quad \text{(Distribution is left-skewed) [cite: 341, 343]} $$
    [cite_start]$$ C_k = \frac{E[(X-\mu)^4]}{\sigma^4} = \frac{12}{5} \quad \text{(Platykurtic) [cite: 341, 344]} $$
\end{enumerate}

\section{Applications}
Gaussian Random Variables are used to model:
\begin{itemize}
    [cite_start]\item \textbf{Thermal Noise:} Noise voltage in electronic circuits[cite: 314].
    [cite_start]\item \textbf{Measurement Error:} Sensor readings where Measured Value = True Value + Gaussian noise[cite: 315].
    [cite_start]\item \textbf{Jitter:} Packet delay variation in communication networks[cite: 316].
    [cite_start]\item \textbf{Density Estimation:} Using sample statistics (histogram) to estimate the distribution parameters $\hat{\mu}$ and $\hat{\sigma}$[cite: 397, 404].
\end{itemize}

\end{document}