\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\title{CSE400 - Fundamentals of Probability in Computing\\
\large Lecture 8: Gaussian, Uniform, Exponential, and Gamma Random Variables}
\author{Scribe: Fagun Rathod - AU2440111}
\date{}

\begin{document}

\maketitle

\section*{Topic 1: Continuous Random Variable (CRV) Moments}

\subsection*{1. Definitions and Notation}
\begin{itemize}
    \item \textbf{$n^{th}$ order Central Moment}: $E[(X - \mu)^n]$.
    \item \textbf{Mean ($\mu$)}: $E[X]$.
    \item \textbf{Variance ($\sigma^2$)}: The second central moment ($n=2$), defined as $E[(X - \mu)^2]$.
    \item \textbf{Skewness ($\gamma_1$)}: The third-order normalized central moment ($\frac{\mu_3}{\sigma^3}$), defined as $\frac{E[(X - \mu)^3]}{\sigma^3}$.
    \item \textbf{Kurtosis ($\beta_2$)}: The fourth central moment ($n=4$), defined as $E[(X - \mu)^4]$.
\end{itemize}

\subsection*{2. Main Results / Theorems}
\begin{itemize}
    \item \textbf{Central Moment for $n=0$}: $E[(X - \mu)^0] = 1$.
    \item \textbf{Central Moment for $n=1$}: $E[(X - \mu)^1] = 0$.
    \item \textbf{Variance Identity}: $\sigma^2 = E[X^2] - (E[X])^2$.
    \item \textbf{Skewness Interpretation}:
    \begin{itemize}
        \item Positive (+) value: PDF is Right Skewed.
        \item Negative (-) value: PDF is Left Skewed.
    \end{itemize}
    \item \textbf{Kurtosis Interpretation}: A large value indicates the RV $X$ will have a large peak near the mean.
    \item \textbf{Linearity of Expectation}: For any constants $a$ and $b$, $E[aX + b] = aE[X] + b$.
    \item \textbf{Expectation of Sums}: For any function $g(X)$, $E[\sum g(x)] = \sum E[g(x)]$.
\end{itemize}

\subsection*{3. Proofs / Derivations}
\textbf{Derivation of Variance Identity ($E[X^2] - \mu^2$)}:
\begin{itemize}
    \item \textbf{Step 1}: Expand the quadratic term: $E[(X - \mu)^2] = E[X^2 - 2\mu X + \mu^2]$.
    \item \textbf{Step 2}: Apply linearity of expectation: $E[X^2] - 2\mu E[X] + E[\mu^2]$.
    \item \textbf{Step 3}: Substitute $E[X] = \mu$: $E[X^2] - 2\mu^2 + \mu^2 = E[X^2] - \mu^2$.
\end{itemize}

\hr

\section*{Topic 2: Gaussian Random Variable}

\subsection*{1. Definitions and Notation}
\begin{itemize}
    \item \textbf{Gaussian PDF}: A random variable $X$ is Gaussian if its PDF is:
    \[ f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \]
    \item \textbf{Notation}: $X \sim N(\mu, \sigma^2)$ where $\mu$ is the mean and $\sigma^2$ is the variance.
    \item \textbf{Standard Normal Distribution}: A Gaussian distribution where $\mu = 0$ and $\sigma = 1$.
\end{itemize}

\subsection*{2. Standard Forms: Q-function and Error Functions}
\begin{itemize}
    \item \textbf{Error Function}: $erf(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt$.
    \item \textbf{Complementary Error Function}: $erfc(x) = 1 - erf(x)$.
    \item \textbf{$\Phi$-function (CDF of Standard Normal)}: $\Phi(x) = P(Z \le x)$.
    \item \textbf{Q-function (Gaussian Tail Function)}: $Q(x) = P(Z > x)$.
\end{itemize}

\subsection*{3. Main Results / Theorems}
\begin{itemize}
    \item \textbf{Identity}: $\Phi(x) + Q(x) = 1$.
    \item \textbf{Evaluating Gaussian CDF}: $F_X(x) = \Phi\left(\frac{x-\mu}{\sigma}\right)$.
    \item \textbf{Evaluating Tail Probabilities}: $P(X > x) = Q\left(\frac{x-\mu}{\sigma}\right)$.
    \item \textbf{CDF in terms of Q-function}: $F_X(x) = 1 - Q\left(\frac{x-\mu}{\sigma}\right)$.
    \item \textbf{Symmetry}: $Q(-x) = 1 - Q(x) = \Phi(x)$.
\end{itemize}

\subsection*{4. Worked Examples}
\textbf{Example 1: Probability Calculation} \\
Given a PDF: $f_X(x) = \frac{1}{\sqrt{18\pi}} e^{-\frac{(x-10)^2}{18}}$. Find probabilities in terms of $\Phi$-functions.
\begin{itemize}
    \item \textbf{Step 1 (Identify Parameters)}: By comparison with the general form, $\mu = 10$ and $2\sigma^2 = 18 \implies \sigma = 3$.
    \item \textbf{Step 2 ($P(X \le 13)$)}: $\Phi\left(\frac{13-10}{3}\right) = \Phi(1)$.
    \item \textbf{Step 3 ($P(X \le 7)$)}: $\Phi\left(\frac{7-10}{3}\right) = \Phi(-1)$.
    \item \textbf{Step 4 ($P(7 \le X \le 13)$)}: This implies $\Phi(1) - \Phi(-1)$.
    \item \textbf{Step 5 ($P(|X-10| \ge 6)$)}: This implies $X \ge 16$ or $X \le 4$. $Q\left(\frac{16-10}{3}\right) + \Phi\left(\frac{4-10}{3}\right) = Q(2) + \Phi(-2)$.
\end{itemize}

\hr

\section*{Topic 3: CDF Analysis Problem}

\subsection*{1. Worked Examples}
\textbf{Example 2: Analyzing a non-Gaussian CDF} Given $F_X(x) = 1 - e^{-2x}$
\begin{itemize}
    \item \textbf{Step 1 (Find PDF)}: $f_X(x) = 2e^{-2x}$ for $x \ge 0$.
    \item \textbf{Step 2 (Mean)}: $E[X] = \frac{1}{2}$.
    \item \textbf{Step 3 (Variance)}: $Var(X) = \frac{1}{4}$.
    \item \textbf{Step 4 (Skewness)}: $\gamma_1 = 2$ (Left-skewed).
    \item \textbf{Step 5 (Kurtosis)}: $\beta_2 = 6$ (Platykurtic).
\end{itemize}

\hr

\section*{Topic 4: Applications in Computing}

\subsection*{1. Definitions and Notation}
\begin{itemize}
    \item \textbf{Thermal Noise}: Random voltage in electronic circuits.
    \item \textbf{Jitter}: Packet delay variation in communication networks.
    \item \textbf{Generative Formula}: $X = \sigma Z + \mu$ (Transforming standard randomness $Z$ into physical model $X$).
    \item \textbf{Sample Statistics}: Using $\bar{x}$ and $s^2$ from raw noisy samples (histograms) to estimate a PDF.
\end{itemize}

\subsection*{2. Main Results / Theorems}
\begin{itemize}
    \item \textbf{Sensor Modeling}: Measured Value = True Value + Gaussian Noise.
    \item \textbf{Image Denoising}: Using Gaussian priors to distinguish real light from random thermal grain.
    \item \textbf{Tail Latency}: Modeling the "99th percentile" in distributed systems for SRE/DevOps.
    \item \textbf{Sensor Fusion (Kalman Filters)}: Guessing real locations between noisy GPS pings.
\end{itemize}

\end{document}